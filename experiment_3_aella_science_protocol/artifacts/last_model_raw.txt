```json
{"files": [{"path": "analysis/code/eda_weighted_summaries.py", "content": "#!/usr/bin/env python3\n\"\"\"Generate exploratory (design-aware) summaries for key survey outcomes.\n\nThis script is deterministic and intended for reproducible exploratory analysis.\nIt produces:\n  1. A summary table with weighted descriptive statistics per outcome.\n  2. A distribution table with weighted response frequencies (small-cell suppressed).\n\nExample:\npython analysis/code/eda_weighted_summaries.py \\\n    --dataset childhoodbalancedpublic_original.csv \\\n    --codebook docs/codebook.json \\\n    --config config/agent_config.yaml \\\n    --out-summary tables/exploratory_outcome_summary.csv \\\n    --out-distribution tables/exploratory_outcome_distribution.csv\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport math\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Iterable, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport yaml\n\n\nDEFAULT_OUTCOMES: Tuple[str, ...] = (\n    \"I love myself (2l8994l)\",\n    \"I tend to suffer from depression (wz901dj)\",\n    \"I tend to suffer from anxiety (npvfh98)-neg\",\n)\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Create exploratory weighted summaries for key survey outcomes.\"\n    )\n    parser.add_argument(\"--dataset\", required=True, help=\"Path to survey microdata (CSV/Parquet).\")\n    parser.add_argument(\"--codebook\", required=True, help=\"Path to JSON codebook for labels.\")\n    parser.add_argument(\"--config\", required=True, help=\"Path to agent configuration YAML.\")\n    parser.add_argument(\n        \"--out-summary\",\n        default=\"tables/exploratory_outcome_summary.csv\",\n        help=\"Output CSV path for summary statistics.\",\n    )\n    parser.add_argument(\n        \"--out-distribution\",\n        default=\"tables/exploratory_outcome_distribution.csv\",\n        help=\"Output CSV path for response distributions.\",\n    )\n    parser.add_argument(\n        \"--outcomes\",\n        nargs=\"+\",\n        default=list(DEFAULT_OUTCOMES),\n        help=\"Column names to summarise. Defaults to key outcomes tracked in hypotheses.\",\n    )\n    parser.add_argument(\n        \"--weight\",\n        default=None,\n        help=\"Optional weight column name. If omitted or null in survey design, uniform weights are used.\",\n    )\n    parser.add_argument(\n        \"--timestamp\",\n        default=datetime.now(tz=timezone.utc).isoformat(timespec=\"seconds\"),\n        help=\"Timestamp metadata recorded in outputs (default is current UTC time).\",\n    )\n    return parser.parse_args()\n\n\ndef load_codebook_labels(codebook_path: Path) -> dict:\n    with codebook_path.open(\"r\", encoding=\"utf-8\") as fh:\n        codebook = json.load(fh)\n    label_map = {entry[\"name\"]: entry.get(\"label\", entry[\"name\"]) for entry in codebook.get(\"variables\", [])}\n    return label_map\n\n\ndef load_config(config_path: Path) -> dict:\n    with config_path.open(\"r\", encoding=\"utf-8\") as fh:\n        return yaml.safe_load(fh)\n\n\ndef weighted_quantiles(values: np.ndarray, weights: np.ndarray, quantiles: Iterable[float]) -> List[float]:\n    if values.size == 0:\n        return [math.nan for _ in quantiles]\n    order = np.argsort(values)\n    sorted_values = values[order]\n    sorted_weights = weights[order]\n    cumulative = np.cumsum(sorted_weights)\n    if cumulative[-1] == 0:\n        return [math.nan for _ in quantiles]\n    cumulative /= cumulative[-1]\n    return [float(np.interp(q, cumulative, sorted_values)) for q in quantiles]\n\n\ndef ensure_parent(path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef main() -> None:\n    args = parse_args()\n\n    dataset_path = Path(args.dataset)\n    if not dataset_path.exists():\n        raise SystemExit(f\"Dataset not found: {dataset_path}\")\n\n    codebook_labels = load_codebook_labels(Path(args.codebook))\n    config = load_config(Path(args.config))\n    small_cell_threshold = config.get(\"small_cell_threshold\", 10)\n    seed = config.get(\"seed\")\n\n    df = pd.read_csv(dataset_path, low_memory=False)\n\n    outcomes = list(dict.fromkeys(args.outcomes))  # preserve order, drop duplicates\n    missing_columns = [col for col in outcomes if col not in df.columns]\n    if missing_columns:\n        raise SystemExit(f\"Outcome columns missing from dataset: {missing_columns}\")\n\n    weight_col = args.weight\n    if weight_col and weight_col not in df.columns:\n        raise SystemExit(f\"Weight column '{weight_col}' not found in dataset.\")\n\n    weights = pd.Series(1.0, index=df.index)\n    weight_source = \"uniform (SRS assumption)\"\n    if weight_col:\n        weights = pd.to_numeric(df[weight_col], errors=\"coerce\").fillna(0.0)\n        weight_source = weight_col\n\n    summary_records = []\n    distribution_frames = []\n\n    for col in outcomes:\n        series = pd.to_numeric(df[col], errors=\"coerce\")\n        valid_mask = series.notna() & weights.notna()\n        x = series[valid_mask].to_numpy(dtype=float)\n        w = weights[valid_mask].to_numpy(dtype=float)\n\n        weight_sum = float(w.sum()) if w.size else 0.0\n        if weight_sum == 0.0:\n            weighted_mean = math.nan\n            weighted_std = math.nan\n        else:\n            weighted_mean = float(np.average(x, weights=w))\n            weighted_var = float(np.average((x - weighted_mean) ** 2, weights=w))\n            weighted_std = math.sqrt(weighted_var)\n\n        quantile_values = weighted_quantiles(x, w, quantiles=(0.1, 0.25, 0.5, 0.75, 0.9))\n\n        summary_records.append(\n            {\n                \"variable\": col,\n                \"label\": codebook_labels.get(col, col),\n                \"n_unweighted\": int(valid_mask.sum()),\n                \"n_weighted\": weight_sum,\n                \"weighted_mean\": weighted_mea\nn,\n                \"weighted_sd\": weighted_std,\n                \"p10\": quantile_values[0],\n                \"p25\": quantile_values[1],\n                \"median\": quantile_values[2],\n                \"p75\": quantile_values[3],\n                \"p90\": quantile_values[4],\n            }\n        )\n\n        # Weighted distribution with small-cell suppression\n        value_counts = (\n            pd.DataFrame({\"value\": series, \"weight\": weights})\n            .dropna()\n            .groupby(\"value\", as_index=False)[\"weight\"]\n            .sum()\n        )\n        total_w = float(value_counts[\"weight\"].sum())\n\n        value_counts[\"proportion\"] = value_counts[\"weight\"] / total_w if total_w else math.nan\n        value_counts[\"count_display\"] = value_counts[\"weight\"].apply(\n            lambda v: \"<10 (suppressed)\" if v < small_cell_threshold else f\"{v:.0f}\"\n        )\n        value_counts[\"proportion_display\"] = value_counts.apply(\n            lambda row: \"\" if isinstance(row[\"count_display\"], str) and \"suppressed\" in row[\"count_display\"]\n            else f\"{row['proportion']:.4f}\",\n            axis=1,\n        )\n        value_counts[\"variable\"] = col\n        value_counts[\"label\"] = codebook_labels.get(col, col)\n        value_counts[\"weight_source\"] = weight_source\n        value_counts[\"small_cell_threshold\"] = small_cell_threshold\n        value_counts[\"seed\"] = seed\n        value_counts[\"timestamp_utc\"] = args.timestamp\n\n        distribution_frames.append(\n            value_counts[\n                [\n                    \"variable\",\n                    \"label\",\n                    \"value\",\n                    \"count_display\",\n                    \"proportion_display\",\n                    \"weight_source\",\n                    \"small_cell_threshold\",\n                    \"seed\",\n                    \"timestamp_utc\",\n                ]\n            ]\n        )\n\n    summary_df = pd.DataFrame(summary_records)\n    summary_df[\"weight_source\"] = weight_source\n    summary_df[\"seed\"] = seed\n    summary_df[\"timestamp_utc\"] = args.timestamp\n\n    ensure_parent(Path(args.out_summary))\n    ensure_parent(Path(args.out_distribution))\n    summary_df.to_csv(args.out_summary, index=False)\n    pd.concat(distribution_frames, ignore_index=True).to_csv(args.out_distribution, index=False)\n\n    print(\n        json.dumps(\n            {\n                \"script\": \"analysis/code/eda_weighted_summaries.py\",\n                \"dataset\": str(dataset_path),\n                \"weight_source\": weight_source,\n                \"outcomes\": outcomes,\n                \"outputs\": [args.out_summary, args.out_distribution],\n                \"seed\": seed,\n                \"timestamp_utc\": args.timestamp,\n            },\n            indent=2,\n        )\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "mode": "text"}, {"path": "tables/exploratory_outcome_summary.csv", "content": "variable,label,n_unweighted,n_weighted,weighted_mean,weighted_sd,p10,p25,median,p75,p90,weight_source,seed,timestamp_utc\nI love myself (2l8994l),Self-love Likert,14436,14436.0,0.6131199778331948,1.8620749225509206,-2.0,-1.0,1.0,2.0,3.0,uniform (SRS assumption),20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,14438,14438.0,-0.40739714641917163,2.0893215593886647,-3.0,-2.0,-1.0,2.0,3.0,uniform (SRS assumption),20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),14437,14437.0,-0.8274572279559466,2.030917021745713,-3.0,-3.0,-1.0,1.0,3.0,uniform (SRS assumption),20251016,2025-11-03T20:34:14+00:00\n", "mode": "text"}, {"path": "tables/exploratory_outcome_distribution.csv", "content": "variable,label,value,count_display,proportion_display,weight_source,small_cell_threshold,seed,timestamp_utc\nI love myself (2l8994l),Self-love Likert,-3.0,1183,0.0819,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,-2.0,1399,0.0969,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,-1.0,1499,0.1038,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,0.0,1817,0.1259,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,1.0,2741,0.1899,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,2.0,3435,0.2379,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI love myself (2l8994l),Self-love Likert,3.0,2362,0.1636,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,-3.0,2786,0.1930,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,-2.0,2596,0.1798,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,-1.0,3133,0.2170,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,0.0,1039,0.0720,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,1.0,1065,0.0738,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,2.0,1721,0.1192,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from depression (wz901dj),Depression tendency,3.0,2098,0.1453,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),-3.0,3780,0.2618,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendenc\ny (neg-coded),-2.0,2919,0.2022,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),-1.0,3150,0.2182,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),0.0,764,0.0529,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),1.0,810,0.0561,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),2.0,1470,0.1018,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\nI tend to suffer from anxiety (npvfh98)-neg,Anxiety tendency (neg-coded),3.0,1544,0.1069,uniform (SRS assumption),10,20251016,2025-11-03T20:34:14+00:00\n", "mode": "text"}, {"path": "notebooks/research_notebook.md", "content": "# Research Notebook\nUpdated: 2025-11-03T20:01:43Z | Seed: 20251016\n\nReproducibility: run `python analysis/code/bootstrap_setup.py`; env info in `artifacts/session_info.txt`; checksums in `artifacts/checksums.json`.\n\nHypotheses: HYP-001 childhood class \u2191 self-love; HYP-002 current class \u2193 depression; HYP-003 CSA \u2191 anxiety; HYP-004 support \u2191 self-love.\n\n2025-11-03T20:29Z \u2014 Survey design validation confirms no sampling weights/strata/clusters present in `childhoodbalancedpublic_original.csv`; working under simple random sampling assumption. See `qc/survey_design_validation.md` regenerated via `python analysis/code/validate_survey_design.py`.\n\n2025-11-03T20:34Z (Exploratory) \u2014 Generated outcome distributions under SRS assumption via\\\n `python analysis/code/eda_weighted_summaries.py --dataset childhoodbalancedpublic_original.csv --codebook docs/codebook.json --config config/agent_config.yaml --out-summary tables/exploratory_outcome_summary.csv --out-distribution tables/exploratory_outcome_distribution.csv`.\\\n Key findings: self-love skews positive (median=1), while depression/anxiety skew negative (median -1). All categories exceed the small-cell threshold (10). Outputs: `tables/exploratory_outcome_summary.csv`, `tables/exploratory_outcome_distribution.csv`.\n\nTODO: document literature and exploratory findings (labelled \"Exploratory\"). Prepare `papers/main/MANIFEST.md` when manuscript drafting begins.\n", "mode": "text"}, {"path": "analysis/decision_log.csv", "content": "ts,action,inputs,rationale_short,code_path,outputs,status\n2025-11-03T20:01:43Z,bootstrap,docs/codebook.json;docs/survey_design.yaml;config/agent_config.yaml,\"Initialized bootstrap scaffolding, hypotheses, PAP draft, and regen script.\",N/A,artifacts/state.json;analysis/hypotheses.csv;analysis/pre_analysis_plan.md,success\n2025-11-03T20:18:32Z,validate_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Confirmed no survey weights; default to SRS.\",analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:23:05Z,regen_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Regenerated design artifacts after script refactor.\",analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:24:59Z,regen_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Refined validation script for leaner reproducibility footprint.\",analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:26:59Z,regen_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Re-executed lean validation script to refresh artifacts.\",analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:29:10Z,regen_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Compressed validation script and refreshed outputs.\",analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:29:10Z,regen_design,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,Compressed validation script and refreshed outputs.,analysis/code/validate_survey_design.py,docs/survey_design.yaml;qc/survey_design_validation.md,success\n2025-11-03T20:34:14Z,exploratory_summary,childhoodbalancedpublic_original.csv;docs/codebook.json;config/agent_config.yaml,\"Generated exploratory outcome summaries under SRS.\",analysis/code/eda_weighted_summaries.py,tables/exploratory_outcome_summary.csv;tables/exploratory_outcome_distribution.csv,success\n", "mode": "text"}, {"path": "artifacts/state.json", "content": "{\n  \"backlog\": [\n    {\n      \"description\": \"Validate survey weights and replicate design\",\n      \"id\": \"T-001\",\n      \"priority\": 1,\n      \"status\": \"completed\"\n    },\n    {\n      \"description\": \"Exploratory weighted summaries for key outcomes\",\n      \"id\": \"T-002\",\n      \"priority\": 1,\n      \"status\": \"completed\"\n    },\n    {\n      \"description\": \"Align codebook schema with dataset\",\n      \"id\": \"T-003\",\n      \"priority\": 2,\n      \"status\": \"pending\"\n    }\n  ],\n  \"bootstrap_complete\": true,\n  \"last_updated\": \"2025-11-03T20:34:30Z\",\n  \"loop_counter\": 2,\n  \"next_actions\": [\n    {\n      \"description\": \"Align codebook schema with dataset\",\n      \"id\": \"T-003\",\n      \"priority\": 2\n    }\n  ],\n  \"notes\": \"Simple random sampling assumption confirmed; exploratory outcome summaries (self-love, depression, anxiety) generated under SRS with command logged in notebook; next focus on schema alignment.\",\n  \"pap_status\": \"draft\",\n  \"priority_hypotheses\": [\n    \"HYP-001\",\n    \"HYP-003\"\n  ],\n  \"seed\": 20251016,\n  \"total_loops\": 30\n}\n", "mode": "text"}], "decision_log_row": {"ts": "2025-11-03T20:34:14Z", "action": "exploratory_summary", "inputs": ["childhoodbalancedpublic_original.csv", "docs/codebook.json", "config/agent_config.yaml"], "rationale_short": "Generated exploratory outcome summaries under SRS.", "code_path": "analysis/code/eda_weighted_summaries.py", "outputs": ["tables/exploratory_outcome_summary.csv", "tables/exploratory_outcome_distribution.csv"], "status": "success"}, "next_actions": [{"id": "T-003", "priority": 1, "desc": "Align codebook schema with dataset", "estimate_min": "25m"}], "state_update": {"loop_counter": 2}, "git": {"commit": true, "message": "feat(eda): add exploratory outcome summaries"}, "stop_now": false, "stop_reason": "", "signals": [{"type": "note", "message": "Exploratory SRS-weighted outcome tables saved at tables/; notebook updated with command references."}]}
```