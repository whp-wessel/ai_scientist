#!/usr/bin/env python3\n\"\"\"\nExecute pre-registered confirmatory models for the childhood balanced public survey.\n\nReproducibility\n--------------\nAll randomness is seeded by `config/agent_config.yaml` (seed=20251016).\nRegenerate provisional confirmatory estimates via:\n\n    python analysis/code/confirmatory_models.py \\\n        --dataset data/clean/childhoodbalancedpublic_with_csa_indicator.csv \\\n        --config config/agent_config.yaml \\\n        --survey-design docs/survey_design.yaml \\\n        --results-csv analysis/results.csv \\\n        --hypotheses HYP-001 HYP-003\n\nThe script is deterministic given identical inputs and environment.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport logging\nimport random\nimport re\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport yaml\n\nRESULT_COLUMNS: List[str] = [\n    \"hypothesis_id\",\n    \"model\",\n    \"n_unweighted\",\n    \"n_weighted\",\n    \"estimate\",\n    \"se\",\n    \"ci_low\",\n    \"ci_high\",\n    \"p_value\",\n    \"q_value\",\n    \"effect_size_metric\",\n    \"robustness_passed\",\n    \"limitations\",\n    \"confidence_rating\",\n    \"analysis_timestamp\",\n    \"seed\",\n]\n\n\ndef _needs_quote(name: str) -> bool:\n    return not re.match(r\"^[A-Za-z_][A-Za-z0-9_]*$\", name)\n\n\ndef _quote(name: str) -> str:\n    return f'Q(\"{name}\")'\n\n\n@dataclass(frozen=True)\nclass HypothesisSpec:\n    hypothesis_id: str\n    outcome: str\n    predictor: str\n    controls: Sequence[str]\n    estimand: str\n    effect_size_metric: str\n    model_label: str\n\n    def required_columns(self) -> List[str]:\n        return [self.outcome, self.predictor, *self.controls]\n\n    def formula(self) -> str:\n        lhs = _quote(self.outcome)\n        tokens = [self.predictor if not _needs_quote(self.predictor) else _quote(self.predictor)]\n        for control in self.controls:\n            tokens.append(control if not _needs_quote(control) else _quote(control))\n        rhs = \" + \".join(tokens)\n        return f\"{lhs} ~ {rhs}\"\n\n    def predictor_term(self) -> str:\n        return self.predictor if not _needs_quote(self.predictor) else _quote(self.predictor)\n\n\nHYPOTHESES: Dict[str, HypothesisSpec] = {\n    \"HYP-001\": HypothesisSpec(\n        hypothesis_id=\"HYP-001\",\n        outcome=\"I love myself (2l8994l)\",\n        predictor=\"classchild\",\n        controls=[\"selfage\", \"gendermale\", \"cis\"],\n        estimand=\"Slope of childhood class predicting self-love\",\n        effect_size_metric=\"slope_per_unit\",\n        model_label=\"ols_hc3_srs\",\n    ),\n    \"HYP-003\": HypothesisSpec(\n        hypothesis_id=\"HYP-003\",\n        outcome=\"I tend to suffer from anxiety (npvfh98)-neg\",\n        predictor=\"CSA_score_indicator\",\n        controls=[\"selfage\", \"gendermale\", \"classchild\"],\n        estimand=\"Mean difference in anxiety for any CSA exposure\",\n        effect_size_metric=\"difference_in_means\",\n        model_label=\"ols_hc3_srs\",\n    ),\n}\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Run pre-registered confirmatory models under SRS assumptions.\"\n    )\n    parser.add_argument(\n        \"--dataset\",\n        default=\"data/clean/childhoodbalancedpublic_with_csa_indicator.csv\",\n        help=\"Input dataset with derived variables.\",\n    )\n    parser.add_argument(\n        \"--config\",\n        default=\"config/agent_config.yaml\",\n        help=\"YAML config containing global seed and defaults.\",\n    )\n    parser.add_argument(\n        \"--survey-design\",\n        default=\"docs/survey_design.yaml\",\n        help=\"Survey design metadata (documenting SRS assumption).\",\n    )\n    parser.add_argument(\n        \"--hypotheses\",\n        nargs=\"+\",\n        default=list(HYPOTHESES.keys()),\n        help=\"Hypothesis IDs to evaluate (default: all registered confirmatory hypotheses).\",\n    )\n    parser.add_argument(\n        \"--results-csv\",\n        default=\"analysis/results.csv\",\n        help=\"Path to results CSV to create/update.\",\n    )\n    parser.add_argument(\n        \"--overwrite\",\n        action=\"store_true\",\n        help=\"Overwrite any existing rows for the targeted hypotheses instead of updating in place.\",\n    )\n    parser.add_argument(\n        \"--log-level\",\n        default=\"INFO\",\n        help=\"Logging level (DEBUG, INFO, WARNING, ERROR).\",\n    )\n    return parser.parse_args()\n\n\ndef load_config(path: Path) -> Dict:\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        config = yaml.safe_load(fh)\n    return config or {}\n\n\ndef configure_logging(level: str) -> None:\n    logging.basicConfig(\n        level=getattr(logging, level.upper(), logging.INFO),\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n    )\n\n\ndef seed_everything(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n\n\ndef load_dataset(dataset_path: Path, required_columns: Iterable[str]) -> pd.DataFrame:\n    if dataset_path.suffix.lower() == \".csv\":\n        df = pd.read_csv(dataset_path)\n    elif dataset_path.suffix.lower() in {\".parquet\", \".pq\"}:\n        df = pd.read_parquet(dataset_path)\n    else:\n        raise ValueError(f\"Unsupported dataset format: {dataset_path.suffix}\")\n\n    missing = [col for col in required_columns if col not in df.columns]\n    if missing:\n        raise KeyError(f\"Dataset missing required columns: {missing}\")\n    return df\n\n\ndef run_model_for_spec(df: pd.DataFrame, spec: HypothesisSpec) -> Dict[str, object]:\n    logging.info(\"Running model for %s\", spec.hypothesis_id)\n\n    subset = df.loc[:, spec.required_columns()].copy()\n    before_drop = len(subset)\n    subset = subset.dropna()\n    dropped = before_drop - len(subset)\n    if dropped:\n        logging.info(\n            \"Dropped %d rows with missing data for %s (%.2f%% of subset).\",\n            dropped,\n            spec.hypothesis_id,\n            dropped / before_drop * 100 if before_drop else 0,\n        )\n\n    if subset.empty:\n        raise ValueError(f\"No rows remain after dropna for {spec.hypothesis_id}.\")\n\n    formula = spec.formula()\n    model = smf.ols(formula=formula, data=subset)\n    fitted = model.fit()\n    robust = fitted.get_robustcov_results(cov_type=\"HC3\")\n\n    term = spec.predictor_term()\n    exog_names = list(robust.model.exog_names)\n    params = pd.Series(robust.params, index=exog_names)\n    bse = pd.Series(robust.bse, index=exog_names)\n    conf_int = pd.DataFrame(\n        robust.conf_int(alpha=0.05),\n        index=exog_names,\n        columns=[\"ci_low\", \"ci_high\"],\n    )\n    pvalues = pd.Series(robust.pvalues, index=exog_names)\n\n    estimate = params[term]\n    se = bse[term]\n    ci_low = conf_int.loc[term, \"ci_low\"]\n    ci_high = conf_int.loc[term, \"ci_high\"]\n    p_value = pvalues[term]\n\n    n_unweighted = len(subset)\n\n    result = {\n        \"hypothesis_id\": spec.hypothesis_id,\n        \"model\": spec.model_label,\n        \"n_unweighted\": n_unweighted,\n        \"n_weighted\": float(n_unweighted),\n        \"estimate\": float(estimate),\n        \"se\": float(se),\n        \"ci_low\": float(ci_low),\n        \"ci_high\": float(ci_high),\n        \"p_value\": float(p_value),\n        \"q_value\": None,\n        \"effect_size_metric\": spec.effect_size_metric,\n        \"robustness_passed\": \"N\",\n        \"limitations\": \"\",\n        \"confidence_rating\": \"Pending\",\n    }\n    return result\n\n\ndef update_results_csv(results_csv: Path, rows: List[Dict[str, object]], seed: int, overwrite: bool) -> None:\n    timestamp = datetime.now(tz=timezone.utc).isoformat()\n    df_new = pd.DataFrame(rows)\n    df_new[\"analysis_timestamp\"] = timestamp\n    df_new[\"seed\"] = seed\n\n    if results_csv.exists():\n        existing = pd.read_csv(results_csv)\n    else:\n        existing = pd.DataFrame(columns=RESULT_COLUMNS)\n\n    existing = existing[[c for c in existing.columns if c in RESULT_COLUMNS]]\n\n    if overwrite:\n        existing = existing[~existing[\"hypothesis_id\"].isin(df_new[\"hypothesis_id\"])]\n    else:\n        mask = existing[\"hypothesis_id\"].isin(df_new[\"hypothesis_id\"])\n        existing = existing[~mask]\n\n    combined = pd.concat([existing, df_new], ignore_index=True)\n    combined = combined[RESULT_COLUMNS]\n    combined.sort_values(by=\"hypothesis_id\", inplace=True)\n    results_csv.parent.mkdir(parents=True, exist_ok=True)\n    combined.to_csv(results_csv, index=False)\n    logging.info(\"Wrote results to %s\", results_csv)\n\n\ndef validate_design_assumption(survey_design_path: Path) -> None:\n    if not survey_design_path.exists():\n        raise FileNotFoundError(\n            f\"Survey design file {survey_design_path} is required to document SRS assumption.\"\n        )\n    with survey_design_path.open(\"r\", encoding=\"utf-8\") as fh:\n        design = yaml.safe_load(fh)\n    assumed = design.get(\"assumed_design\")\n    if assumed != \"simple_random_sampling\":\n        raise ValueError(\n            \"Confirmatory models currently implemented only for simple random sampling. \"\n            f\"Found assumed_design={assumed!r}.\"\n        )\n\n\ndef main() -> None:\n    args = parse_args()\n    configure_logging(args.log_level)\n\n    config = load_config(Path(args.config))\n    seed = int(config.get(\"seed\", 0) or 0)\n    if seed <= 0:\n        raise ValueError(\"Global seed must be a positive integer in config/agent_config.yaml.\")\n    seed_everything(seed)\n    logging.info(\"Global seed set to %d\", seed)\n\n    validate_design_assumption(Path(args.survey_design))\n\n    requested = args.hypotheses\n    unknown = [hyp for hyp in requested if hyp not in HYPOTHESES]\n    if unknown:\n        raise KeyError(f\"Hypotheses not registered in confirmatory registry: {unknown}\")\n\n    specs = [HYPOTHESES[hyp] for hyp in requested]\n    required_columns = sorted({col for spec in specs for col in spec.required_columns()})\n    df = load_dataset(Path(args.dataset), required_columns)\n\n    results: List[Dict[str, object]] = []\n    for spec in specs:\n        row = run_model_for_spec(df, spec)\n        results.append(row)\n\n    update_results_csv(Path(args.results_csv), results, seed=seed, overwrite=args.overwrite)\n\n\nif __name__ == \"__main__\":\n    main()\n